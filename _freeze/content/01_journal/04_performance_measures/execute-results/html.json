{
  "hash": "e1063364a60ce6cd2a55fb35648cf324",
  "result": {
    "markdown": "---\ntitle: \"Performance Measures\"\nauthor: \"Adrian Florea\"\n---\n\n\n# Challenge 5: Visualizations for Product Backorders Dataset after AutoML\n\nFor the same reason as in the previous challenge, to prevent issues, the code here is not run directly. The resulting plots are imported. Once again, for the full computation steps, refer to the corresponding `.R` file `performance_measures_CHALLENGE.R` located in folder `...\\CHALLENGES`.\n\n# Leaderboard visualization\nA broad overview of the top 15 computed models. The results look already highly accurate, which might be because of a more powerful system on my side. We can see that stacked ensemble algorithms are the best performing ones.\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-1_df9738e24a9f4513fb0d5c02af1e8935'}\n\n```{.r .cell-code}\nlibrary(h2o)\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(cowplot)\nlibrary(glue)\n\nh2o.init()\n\n# We need the data from the previous challenge, so either run source() or\n# directly run this file if the previous challenge file already ran before\n\n# source(\"CHALLENGES/automated_ml_II_CHALLENGE.R\")\n\n# 1 Leaderboard visualization ----\n\n# Function for plotting hte leaderboard\n\nplot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c(\"auc\", \"logloss\"), \n                                 n_max = 20, size = 4, include_lbl = TRUE) {\n  \n  # Setup inputs\n  # adjust input so that all formats are working\n  order_by <- tolower(order_by[[1]])\n  \n  leaderboard_tbl <- h2o_leaderboard %>%\n    as.tibble() %>%\n    select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% \n    mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n    rownames_to_column(var = \"rowname\") %>%\n    mutate(model_id = paste0(rowname, \". \", model_id) %>% as.factor())\n  \n  # Transformation\n  if (order_by == \"auc\") {\n    \n    data_transformed_tbl <- leaderboard_tbl %>%\n      slice(1:n_max) %>%\n      mutate(\n        model_id   = as_factor(model_id) %>% reorder(auc),\n        model_type = as.factor(model_type)\n      ) %>%\n      pivot_longer(cols = -c(model_id, model_type, rowname), \n                   names_to = \"key\", \n                   values_to = \"value\", \n                   names_transform = list(key = forcats::fct_inorder)\n      )\n    \n  } else if (order_by == \"logloss\") {\n    \n    data_transformed_tbl <- leaderboard_tbl %>%\n      slice(1:n_max) %>%\n      mutate(\n        model_id   = as_factor(model_id) %>% reorder(logloss) %>% fct_rev(),\n        model_type = as.factor(model_type)\n      ) %>%\n      pivot_longer(cols = -c(model_id, model_type, rowname), \n                   names_to = \"key\", \n                   values_to = \"value\", \n                   names_transform = list(key = forcats::fct_inorder)\n      )\n    \n  } else {\n    # If nothing is supplied\n    stop(paste0(\"order_by = '\", order_by, \"' is not a permitted option.\"))\n  }\n  \n  # Visualization\n  g <- data_transformed_tbl %>%\n    ggplot(aes(value, model_id, color = model_type)) +\n    geom_point(size = size) +\n    facet_wrap(~ key, scales = \"free_x\") +\n    labs(title = \"Leaderboard Metrics\",\n         subtitle = paste0(\"Ordered by: \", toupper(order_by)),\n         y = \"Model Postion, Model ID\", x = \"\")\n  \n  if (include_lbl) g <- g + geom_label(aes(label = round(value, 2), \n                                           hjust = \"inward\"))\n  \n  return(g)\n  \n}\n\nplot_h2o_leaderboard(automl_models_h2o@leaderboard,\"auc\", 15, 3, T)\n\n# Save plot for journal\nggsave(\"CHALLENGES/performance_measures_files/plot1.png\", width = 25, height = 20, units = \"cm\")\n```\n:::\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-2_edee95faa17e907c14835107c31ce2d8'}\n::: {.cell-output-display}\n![](../../CHALLENGES/performance_measures_files/plot1.png){width=1476}\n:::\n:::\n\n\n# Tuning a model with grid search\nIf one wants to improve their models, the easiest way is to just allow more computing time. However, it is also possible to fine tune a model by making a \"grid\" of various parameter combinations that are used to constrain the model during training. In this case, we can use a small amount of so called hyperparameters to squeeze out a bit more performance out of our model.\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-3_40dd9c3d9b77a83aa7c7e5cee764073f'}\n\n```{.r .cell-code}\n# 2 Tune a model with grid search ----\n\n# May have to change number whenever models are recomputed\n# Let's refine a GBM model here\ngbm_h2o <- automl_models_h2o@leaderboard %>% \n  extract_h2o_model_name_by_position(6) %>% \n  h2o.getModel()\n\n# Checking the model: 3.58% error for training set\ngbm_h2o\n\n# For the test set we get ~8.85% error\nh2o.performance(gbm_h2o, newdata = as.h2o(test_tbl))\n\n# Save for journal\ngbm_h2o %>% \n  h2o.performance(newdata = as.h2o(test_tbl)) %>% \n  h2o.confusionMatrix() %>% \n  as_tibble() %>% \n  saveRDS(\"CHALLENGES/performance_measures_files/gbm_untuned.rds\")\n\ngbm_grid_01 <- h2o.grid(\n  \n  # See help page for available algos\n  algorithm = \"gbm\",\n  \n  # I just use the same as the object\n  grid_id = \"gbm_grid_01\",\n  \n  x = x,\n  y = y,\n  \n  # training and validation frame and crossfold validation\n  training_frame   = train_h2o,\n  validation_frame = valid_h2o,\n  nfolds = 5,\n  \n  hyper_params = list(\n    max_depth = c(9, 10, 11, 12),\n    learn_rate = c(0.1, 0.13, 0.15, 0.17)\n  )\n)\n\nh2o.getGrid(grid_id = \"gbm_grid_01\", sort_by = \"auc\", decreasing = TRUE)\n\n# Check which model ID was the best\ngbm_grid_01_model_best <- h2o.getModel(\"gbm_grid_01_model_31\")\n\n# Noticeable difference but not too overfit\ngbm_grid_01_model_best %>% h2o.auc(train = T, valid = T, xval = T)\n\n# On the test data, we now get an error of 7.7%, which is a small improvement!\ngbm_grid_01_model_best %>%\n  h2o.performance(newdata = as.h2o(test_tbl))\n\n# Save for journal\ngbm_grid_01_model_best %>% \n  h2o.performance(newdata = as.h2o(test_tbl)) %>% \n  h2o.confusionMatrix() %>% \n  as_tibble() %>% \n  saveRDS(\"CHALLENGES/performance_measures_files/gbm_tuned.rds\")\n```\n:::\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-4_b88a2eb8f4428e605439c6f8f012107f'}\n\n```{.r .cell-code}\ngbm_untuned <- readRDS(\"~/GitHub/ss24-bdml-Adrian-0402/CHALLENGES/performance_measures_files/gbm_untuned.rds\")\ngbm_tuned <- readRDS(\"~/GitHub/ss24-bdml-Adrian-0402/CHALLENGES/performance_measures_files/gbm_tuned.rds\")\n\n# Before tuning:\ngbm_untuned\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"No\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Yes\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Rate\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"2300\",\"2\":\"180\",\"3\":\"0.07258065\",\"4\":\"=180/2480\"},{\"1\":\"73\",\"2\":\"305\",\"3\":\"0.19312169\",\"4\":\"=73/378\"},{\"1\":\"2373\",\"2\":\"485\",\"3\":\"0.08852344\",\"4\":\"=253/2858\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# After tuning:\ngbm_tuned\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"No\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Yes\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Rate\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"2364\",\"2\":\"116\",\"3\":\"0.04677419\",\"4\":\"=116/2480\"},{\"1\":\"104\",\"2\":\"274\",\"3\":\"0.27513228\",\"4\":\"=104/378\"},{\"1\":\"2468\",\"2\":\"390\",\"3\":\"0.07697691\",\"4\":\"=220/2858\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n# Trade-off between precision and recall with optimal threshold\nThis type of plot shows us the optimal balance of precision and recall defined by a threshold. Generally speaking, we compare false positives (precision) to false negatives (recall) with `1` being the ideal (no incorrect classifications). Since we can not have the best of both worlds, we need to find a good balance, hence the threshold. However this is not the best answer since false negatives are usually more costly for the business case.\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-5_cab7599da343eed34742f143e13c0f6b'}\n\n```{.r .cell-code}\n# 3 Trade-off between precision and recall with optimal threshold ----\n\n# Lets load 3 different models for future plots\ngbm_h2o <- automl_models_h2o@leaderboard %>% \n  extract_h2o_model_name_by_position(6) %>% \n  h2o.getModel()\n\nstacked_ensemble_h2o <- automl_models_h2o@leaderboard %>% \n  extract_h2o_model_name_by_position(1) %>% \n  h2o.getModel()\n\nxrt_h2o <- automl_models_h2o@leaderboard %>% \n  extract_h2o_model_name_by_position(13) %>% \n  h2o.getModel()\n\n# Save for later plots\ngbm_h2o %>% h2o.saveModel(\"CHALLENGES/performance_measures_files/models/\")\nstacked_ensemble_h2o %>% h2o.saveModel(\"CHALLENGES/performance_measures_files/models/\")\nxrt_h2o %>% h2o.saveModel(\"CHALLENGES/performance_measures_files/models/\")\n\nperformance_h2o <- h2o.performance(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\nperformance_tbl <- performance_h2o %>%\n  h2o.metric() %>%\n  as.tibble() \n\n# Save this theme for later plots\ntheme_new <- theme(\n  legend.position  = \"bottom\",\n  legend.key       = element_blank(),\n  panel.background = element_rect(fill   = \"transparent\"),\n  panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n  panel.grid.major = element_line(color = \"grey\", size = 0.333)\n) \n\nperformance_tbl %>%\n  ggplot(aes(x = threshold)) +\n  geom_line(aes(y = precision), color = \"blue\", size = 1) +\n  geom_line(aes(y = recall), color = \"red\", size = 1) +\n  \n  # Insert line where precision and recall are harmonically optimized\n  geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n  labs(title = \"Precision vs Recall\", y = \"value\") +\n  theme_new\n\n# Save plot for journal\nggsave(\"CHALLENGES/performance_measures_files/plot2.png\", width = 25, height = 15, units = \"cm\")\n```\n:::\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-6_c15a20a0ec756483f7e8689801cc72f2'}\n::: {.cell-output-display}\n![](../../CHALLENGES/performance_measures_files/plot2.png){width=1476}\n:::\n:::\n\n\n# ROC Plot\nThe ROC curve puts true positives against false positives, which can give us a good first impression about the performance of the model.\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-7_768a2a9f10aea35f1e21b6b445136030'}\n\n```{.r .cell-code}\n# 4 ROC Plot ----\n\npath <- \"CHALLENGES/performance_measures_files/models/GBM_4_AutoML_3_20240616_180904\"\n\nload_model_performance_metrics <- function(path, test_tbl) {\n  \n  model_h2o <- h2o.loadModel(path)\n  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n  \n  perf_h2o %>%\n    h2o.metric() %>%\n    as_tibble() %>%\n    mutate(auc = h2o.auc(perf_h2o)) %>%\n    select(tpr, fpr, auc)\n  \n}\n\nmodel_metrics_tbl <- fs::dir_info(path = \"CHALLENGES/performance_measures_files/models/\") %>%\n  select(path) %>%\n  mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%\n  unnest(cols = metrics)\n\nmodel_metrics_tbl %>%\n  mutate(\n    # Extract the model names\n    path = str_split(path, pattern = \"/\", simplify = T)[,4] %>% as_factor(),\n    auc  = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(fpr, tpr, color = path, linetype = auc)) +\n  geom_line(size = 1) +\n  \n  # just for demonstration purposes\n  geom_abline(color = \"red\", linetype = \"dotted\") +\n  \n  theme_new +\n  theme(\n    legend.direction = \"vertical\",\n  ) +\n  labs(\n    title = \"ROC Plot\",\n    subtitle = \"Performance of 3 Top Performing Models for each Algorithm\"\n  )\n\n# Save plot for journal\nggsave(\"CHALLENGES/performance_measures_files/plot3.png\", width = 25, height = 18, units = \"cm\")\n```\n:::\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-8_fdd84873869f782d55d6c189bc72c9a5'}\n::: {.cell-output-display}\n![](../../CHALLENGES/performance_measures_files/plot3.png){width=1476}\n:::\n:::\n\n\n# Precision vs. Recall Plot\nTo visualize the tradeoff between precision and recall, this plot shows how increasing one metric influences the other. Again, it's up to the user to make the appropriate decision.\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-9_1f81867cf96351a93b3273658d4391ec'}\n\n```{.r .cell-code}\n# 5 Precision vs. Recall Plot ----\n\nload_model_performance_metrics <- function(path, test_tbl) {\n  \n  model_h2o <- h2o.loadModel(path)\n  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n  \n  perf_h2o %>%\n    h2o.metric() %>%\n    as_tibble() %>%\n    mutate(auc = h2o.auc(perf_h2o)) %>%\n    select(tpr, fpr, auc, precision, recall)\n  \n}\n\nmodel_metrics_tbl <- fs::dir_info(path = \"CHALLENGES/performance_measures_files/models/\") %>%\n  select(path) %>%\n  mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%\n  unnest(cols = metrics)\n\nmodel_metrics_tbl %>%\n  mutate(\n    path = str_split(path, pattern = \"/\", simplify = T)[,4] %>% as_factor(),\n    auc  = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(recall, precision, color = path, linetype = auc)) +\n  geom_line(size = 1) +\n  theme_new + \n  theme(\n    legend.direction = \"vertical\",\n  ) +\n  labs(\n    title = \"Precision vs Recall Plot\",\n    subtitle = \"Performance of 3 Top Performing Models for each Algorithm\"\n  )\n\n# Save plot for journal\nggsave(\"CHALLENGES/performance_measures_files/plot4.png\", width = 25, height = 18, units = \"cm\")\n```\n:::\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-10_dd68700165b42886e755c8e247963d9c'}\n::: {.cell-output-display}\n![](../../CHALLENGES/performance_measures_files/plot4.png){width=1476}\n:::\n:::\n\n\n# Gain Plot\nThe gain helps us to understand the overall benefit of using models over random choices when making predictions. Assume we have computed a decent model and we determined for a set of values their probabilities (in this case, whether a product will be on backorder or not). Then for the highest probability groups (whatever size they may be) we **gain** the ability to correctly predict the outcome using our model.\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-11_e809d4f3c204f6c225c24afb0e3e2b95'}\n\n```{.r .cell-code}\n# 6 Gain Plot ----\n\ngain_lift_tbl <- performance_h2o %>%\n  h2o.gainsLift() %>%\n  as.tibble()\n\ngain_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"lift\")) %>%\n  mutate(baseline = cumulative_data_fraction) %>%\n  rename(gain     = cumulative_capture_rate) %>%\n  # prepare the data for the plotting (for the color and group aesthetics)\n  pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\n\ngain_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Gain Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Gain\"\n  ) +\n  theme_new\n\n# Save plot for journal\nggsave(\"CHALLENGES/performance_measures_files/plot5.png\", width = 25, height = 20, units = \"cm\")\n```\n:::\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-12_c8da1a8e97ccf0836fa961d2ec89577c'}\n::: {.cell-output-display}\n![](../../CHALLENGES/performance_measures_files/plot5.png){width=1476}\n:::\n:::\n\n\n# Lift Plot\nClosely related to the gain, the lift tells us how many times better our model is over randomly predicting the outcome. This metric is given by the respective gain divided by the expectation.\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-13_aa5d86da17db682a8593bee01f57f634'}\n\n```{.r .cell-code}\n# 7 Lift Plot ----\n\nlift_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"capture\")) %>%\n  mutate(baseline = 1) %>%\n  rename(lift = cumulative_lift) %>%\n  pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n\nlift_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Lift Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Lift\"\n  ) +\n  theme_new\n\n# Save plot for journal\nggsave(\"CHALLENGES/performance_measures_files/plot6.png\", width = 25, height = 20, units = \"cm\")\n```\n:::\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-14_fef190eeec2f9a38fd3fe8486c22b737'}\n::: {.cell-output-display}\n![](../../CHALLENGES/performance_measures_files/plot6.png){width=1476}\n:::\n:::\n\n\n# Dashboard with cowplot\nThe cowplot package can help us put all the above plots into a bigger summed up picture. Having all different aspects and plots of models shown at once, it is easier to come to a conclusion which model shall be finally used for the specific business case.\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-15_49a5daab4b005895dc6ae67c2db95d80'}\n\n```{.r .cell-code}\n# 8 Dashboard with cowplot ----\n\n# Calculate and arrange all previous plots in one big plot\nplot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c(\"auc\", \"logloss\"),\n                                 max_models = 3, size = 1.5) {\n  \n  # Inputs\n  \n  leaderboard_tbl <- h2o_leaderboard %>%\n    as_tibble() %>%\n    slice(1:max_models)\n  \n  newdata_tbl <- newdata %>%\n    as_tibble()\n  \n  # Selecting the first, if nothing is provided\n  order_by      <- tolower(order_by[[1]]) \n  \n  # Convert string stored in a variable to column name (symbol)\n  order_by_expr <- rlang::sym(order_by)\n  \n  # Turn of the progress bars ( opposite h2o.show_progress())\n  h2o.no_progress()\n  \n  # 1. Model metrics\n  \n  get_model_performance_metrics <- function(model_id, test_tbl) {\n    \n    model_h2o <- h2o.getModel(model_id)\n    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))\n    \n    perf_h2o %>%\n      h2o.metric() %>%\n      as.tibble() %>%\n      select(threshold, tpr, fpr, precision, recall)\n    \n  }\n  \n  model_metrics_tbl <- leaderboard_tbl %>%\n    mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%\n    unnest(cols = metrics) %>%\n    mutate(\n      model_id = as_factor(model_id) %>% \n        # programmatically reorder factors depending on order_by\n        fct_reorder(!! order_by_expr, \n                    .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n      auc      = auc %>% \n        round(3) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id)),\n      logloss  = logloss %>% \n        round(4) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id))\n    )\n  \n  \n  # 1A. ROC Plot\n  \n  p1 <- model_metrics_tbl %>%\n    ggplot(aes(fpr, tpr, color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    theme_new +\n    labs(title = \"ROC\", x = \"FPR\", y = \"TPR\") +\n    theme(legend.direction = \"vertical\") \n  \n  \n  # 1B. Precision vs Recall\n  \n  p2 <- model_metrics_tbl %>%\n    ggplot(aes(recall, precision, color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    theme_new +\n    labs(title = \"Precision Vs Recall\", x = \"Recall\", y = \"Precision\") +\n    theme(legend.position = \"none\") \n  \n  \n  # 2. Gain / Lift\n  \n  get_gain_lift <- function(model_id, test_tbl) {\n    \n    model_h2o <- h2o.getModel(model_id)\n    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n    \n    perf_h2o %>%\n      h2o.gainsLift() %>%\n      as.tibble() %>%\n      select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)\n    \n  }\n  \n  gain_lift_tbl <- leaderboard_tbl %>%\n    mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%\n    unnest(cols = metrics) %>%\n    mutate(\n      model_id = as_factor(model_id) %>% \n        fct_reorder(!! order_by_expr, \n                    .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n      auc  = auc %>% \n        round(3) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id)),\n      logloss = logloss %>% \n        round(4) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id))\n    ) %>%\n    rename(\n      gain = cumulative_capture_rate,\n      lift = cumulative_lift\n    ) \n  \n  # 2A. Gain Plot\n  \n  p3 <- gain_lift_tbl %>%\n    ggplot(aes(cumulative_data_fraction, gain, \n               color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size,) +\n    geom_segment(x = 0, y = 0, xend = 1, yend = 1, \n                 color = \"red\", size = size, linetype = \"dotted\") +\n    theme_new +\n    expand_limits(x = c(0, 1), y = c(0, 1)) +\n    labs(title = \"Gain\",\n         x = \"Cumulative Data Fraction\", y = \"Gain\") +\n    theme(legend.position = \"none\")\n  \n  # 2B. Lift Plot\n  \n  p4 <- gain_lift_tbl %>%\n    ggplot(aes(cumulative_data_fraction, lift, \n               color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    geom_segment(x = 0, y = 1, xend = 1, yend = 1, \n                 color = \"red\", size = size, linetype = \"dotted\") +\n    theme_new +\n    expand_limits(x = c(0, 1), y = c(0, 1)) +\n    labs(title = \"Lift\",\n         x = \"Cumulative Data Fraction\", y = \"Lift\") +\n    theme(legend.position = \"none\") \n  \n  \n  # Combine using cowplot\n  \n  # Original code does not plot the legend at all\n  # Need to fix the given code, have to extract legend correctly\n  # First extract all components and then combine them\n  \n  # Extract all legend components\n  legends <- cowplot::get_plot_component(p1, \"guide-box\", return_all = TRUE)\n  \n  # Combine legends if there are multiple\n  p_legend <- cowplot::plot_grid(plotlist = legends, ncol = 1)\n\n  # Remove legend from p1\n  p1 <- p1 + theme(legend.position = \"none\")\n  \n  # cowplot::plt_grid() combines multiple ggplots into a single cowplot object\n  p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n  \n  # cowplot::ggdraw() sets up a drawing layer\n  p_title <- ggdraw() + \n    \n    # cowplot::draw_label() draws text on a ggdraw layer / ggplot object\n    draw_label(\"H2O Model Metrics\", size = 18, fontface = \"bold\", \n               color = \"#2C3E50\")\n  \n  p_subtitle <- ggdraw() + \n    draw_label(glue(\"Ordered by {toupper(order_by)}\"), size = 10,  \n               color = \"#2C3E50\")\n  \n  # Combine everything\n  ret <- plot_grid(p_title, p_subtitle, p, p_legend, \n                   \n                   # Adjust the relative spacing, so that the legends always fits\n                   ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))\n  \n  h2o.show_progress()\n  \n  return(ret)\n  \n}\n\nautoml_models_h2o@leaderboard %>%\n  plot_h2o_performance(newdata = test_tbl, order_by = \"logloss\", \n                       size = 0.5, max_models = 4)\n\n# Save plot for journal\nggsave(\"CHALLENGES/performance_measures_files/plot7.png\", width = 25, height = 25, units = \"cm\")\n```\n:::\n\n::: {.cell hash='04_performance_measures_cache/html/unnamed-chunk-16_ec6ffb782c47df2d7418a5942327e9d9'}\n::: {.cell-output-display}\n![](../../CHALLENGES/performance_measures_files/plot7.png){width=1476}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}